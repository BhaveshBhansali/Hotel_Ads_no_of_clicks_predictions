{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## In this notebook. I have tried different parametric and non-parametric models to predict number of clicks based on characterstics of hotels. I have added some additional features for better predictions.\n",
    "   \n",
    "   \n",
    "\n",
    "1. Feature Engineering: I have added following features\n",
    "\n",
    "     - Number of Hotels (n_hotels): Number of hotels affect the chances of clicking on hotel offers (More choices of offers then number of clicks distributes between those offers)\n",
    "\n",
    "     - Promotion: If price of hotel compared to hotels of same category ('stars') within same cities is less, tha flag promotion as 1 and vice versa\n",
    "     \n",
    "     - Hotel Popularity (hotel popularity from comparable number of reviews): ratio of number_of_reviews(n_reviews)/avg(n_reviews) from hotels of same category ('stars') within same cities.\n",
    "     \n",
    "2. Scaling of features: Since I did not have idea which features should be given more weightage, I have scaled the features so that no feature should dominate other feature (when underlying model use some kind of euclidean distance). However, I also used scaled features in tree based models which do not require scaled features. Nevertheless, algorithm converges faster.\n",
    "     \n",
    "3. Parametric Model: \"Linear Regression\" (The idea of using this model was to check if simple parametric model can be fit with given dataset (to get complexity of the problem))\n",
    "\n",
    "     - The \"NWMSE\" of this model is 1.04\n",
    "     \n",
    "     - Assumption: I convert predictions less than 0.9 to 0.\n",
    "\n",
    "     - High error might be due to violation of model assumptions:\n",
    "         \n",
    "         - NO Linear relationship\n",
    "         - High Multicollinearity: Independent variables are highly positive correlated \n",
    "         - High variance of residuals: varaince of residuals changes with fitted line\n",
    "         - Non-Normality of residuals: Might be due to outliers\n",
    "         \n",
    "     - Let's keep this error as benchmark and try different models.\n",
    "         \n",
    "4. \"Support Vector Regressor with kernel trick\" to get an intution if Non-Linear model could fit data better than linear model.\n",
    "\n",
    "     - I tried it. It was running endlessly. I stopped it.\n",
    "\n",
    "5. Non-Parametric Model: Random Forest\n",
    "     - I have used Randomized search with cross validation to find the best parameters\n",
    "     \n",
    "     - The \"NWMSE\" score comes down to 0.821 \n",
    "     \n",
    "     - The non-parametric,non linear model (RF) works better than.\n",
    "\n",
    "6. Non-Parametric Model: XGBoost\n",
    "\n",
    "     - I have used Randomized search with cross validation to find the best parameters\n",
    "\n",
    "     - The \"NWMSE\" score comes down to 0.723\n",
    "     \n",
    "     - The non-parametric, non linear model (XGBoost) works better than RandomForest and Linear Regression.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#parametric and non parametric\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges:\n",
    "    1. Around 65% rows have number clicks equal to zero.\n",
    "    2. Outlier cases: unexpected number of number of images and number of reviews\n",
    "    3. For first run, I keep data as it is (not handling outlier cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- Normalizing features since different feature has different value range. Our model might influenced heavy by features having high values relative to other features. \n",
    "\n",
    "- Also, I normalize test data here. It should be normalized by mean and variance information from training data. If we build different scaler (from whole dataset together before splitting into test/train/valid data) to scale. Then it means, we add future information into explanatory variables (mean and variance)  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(data_train,data_valid,data_test):\n",
    "    \"\"\"\n",
    "    This function is used to normalize features (training, validation, test data) using explanatory information (mean and variance)\n",
    "    from training data.\n",
    "    \n",
    "    Args:\n",
    "        data_train: training data\n",
    "        data_valid: validation data\n",
    "        data_test: test data\n",
    "        \n",
    "    Returns:\n",
    "        normalized training, validation and test data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    \n",
    "    X_train_transform=scaler.transform(data_train)\n",
    "    \n",
    "    ## training model parameters are used to scale parameters in test data (i.e, model learn to classify less than 6 to class1\n",
    "    ## and if we do different scaling on test data, model could predict wrong). Moreover, our test and train data are from same\n",
    "    ## distribution. It does not matter even if we use different scaling.\n",
    "    X_valid_transform=scaler.transform(data_valid)\n",
    "    \n",
    "    X_test_transform=scaler.transform(data_test)\n",
    "    \n",
    "    return X_train_transform,X_valid_transform,X_test_transform\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_train,df_test,features):\n",
    "    \"\"\"\n",
    "    This function is used to prepare dataset for training the model. Also, it creates validation dataset from \n",
    "    whole dataset. First it split the data into train and validation data.It also calls for \n",
    "    normalize_features function to normalize the features.\n",
    "    \n",
    "    Args:\n",
    "        df_train: pandas dataframe for training data (divided into training and validation data)\n",
    "        df_test: Pandas dataframe of test data\n",
    "        features: list of features\n",
    "        \n",
    "    Returns:\n",
    "        normalized training, validation and test data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    label='n_clicks'\n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(df_train[features],df_train[label],train_size=0.80 ,random_state=0)\n",
    "    norm_X_train,norm_X_valid,norm_X_test=normalize_features(X_train,X_valid,df_test[features])\n",
    "    \n",
    "    print(\"shape of x_train: \"+str(norm_X_train.shape))\n",
    "    print(\"shape of y_train: \"+str(y_train.shape))\n",
    "    print(\"shape of x_valid: \"+str(norm_X_valid.shape))\n",
    "    print(\"shape of y_valid: \"+str(y_valid.shape))\n",
    "    print(\"shape of x_test: \"+str(norm_X_test.shape))\n",
    "    \n",
    "    return norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nwmse_metric(observed, predicted):\n",
    "    \"\"\"\n",
    "    This function is used to compute \"normalized weighted mean squared error\" as defined in the case\n",
    "    study description.\n",
    "    \n",
    "    Args:\n",
    "        observed: original dependent variable (y label)\n",
    "        predicted: predicted dependent variable (y label) \n",
    "        \n",
    "    Returns:\n",
    "        normalized training, validation and test data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    sum_error = 0.0\n",
    "    weight_sum=0\n",
    "    for i in range(len(observed)):\n",
    "        prediction_error = predicted[i] - observed[i]\n",
    "        weight=math.log(observed[i]+1)+1\n",
    "        sum_error += weight*(prediction_error ** 2)\n",
    "        weight_sum+=weight\n",
    "        \n",
    "    norm_weighted_mean_squared_error = sum_error / (float(len(observed))*weight_sum)\n",
    "    \n",
    "    #return math.sqrt(mean_error)\n",
    "    return norm_weighted_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_search_CV(algorithm,tuned_parameters,train_x,train_y,iteration,verbose_int,jobs_int=1):\n",
    "    \"\"\"\n",
    "    This function performs randomized search on set of features, perform cross validation and fit the data in the specified model.\n",
    "    It returns best set of features.\n",
    "    \n",
    "    Args:\n",
    "        algorithm: Model to fit\n",
    "        tuned_parameters: Set of possible parameters to choose\n",
    "        train_x: training data (features) \n",
    "        train_y: training label\n",
    "        iteration: number of itearation to choose random set of parameters\n",
    "        \n",
    "    Returns:\n",
    "        set of best parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    score = 'neg_mean_squared_error'\n",
    "\n",
    "    print(\"# Tuning hyper-parameters for mean_squared_error\" )\n",
    "    print()\n",
    "     \n",
    "    clf = RandomizedSearchCV(algorithm, param_distributions=tuned_parameters, cv=5,n_iter = iteration, scoring='neg_mean_squared_error',verbose=verbose_int,n_jobs=jobs_int)\n",
    "    clf.fit(train_x,train_y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "    There is no missing values in this dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Train data : (395760, 13)\n",
      "Shape Test data : (131929, 12)\n"
     ]
    }
   ],
   "source": [
    "# Reading training & test data \n",
    "df_train=pd.read_csv('train_data_processed.csv')\n",
    "df_test=pd.read_csv('test_data_processed.csv')\n",
    "print('Shape Train data :',df_train.shape)\n",
    "print('Shape Test data :',df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:\n",
    "\n",
    "Let's think, what additional features could be added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features=df_train.copy()\n",
    "df_test_features=df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1. Total Number of hotels in each city (this might affect click chances, high number of hotel offer options for user, then clicks distributed in that fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features=df_train_features.assign(n_hotels=df_train_features.city_id.map(df_train_features.groupby('city_id')['hotel_id'].count()))\n",
    "df_test_features=df_test_features.assign(n_hotels=df_test_features.city_id.map(df_test_features.groupby('city_id')['hotel_id'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2. Relative Popularity of the hotel\n",
    "How popular is the hotel in the city if group by it's stars (popular among hotels of same stars in the city)\n",
    "\n",
    "Hotel Popularity = n_reviews/(sum of n_reviews of hotels of same stars from the same city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features_intermed=df_train_features.groupby(['city_id','stars'])['n_reviews'].sum().reset_index().rename(columns={'n_reviews':'Total_reviews'})\n",
    "df_test_features_intermed=df_test_features.groupby(['city_id','stars'])['n_reviews'].sum().reset_index().rename(columns={'n_reviews':'Total_reviews'})\n",
    "merged_train = pd.merge(df_train_features,df_train_features_intermed, on=['city_id','stars'])\n",
    "merged_test = pd.merge(df_test_features,df_test_features_intermed, on=['city_id','stars'])\n",
    "df_train_features['hotel_popularity_reviews']=merged_train['n_reviews']/merged_train['Total_reviews']\n",
    "df_test_features['hotel_popularity_reviews']=merged_test['n_reviews']/merged_test['Total_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3. Promotion Flag\n",
    "\n",
    "Idea: If avg_price of hotel is less than avg_price of the hotels from same city than it is a promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features_intermed=df_train_features.groupby(['city_id','stars'])['avg_price'].mean().reset_index().rename(columns={'avg_price':'area_star_avg_price'})\n",
    "df_test_features_intermed=df_test_features.groupby(['city_id','stars'])['avg_price'].mean().reset_index().rename(columns={'avg_price':'area_star_avg_price'})\n",
    "merged_train = pd.merge(df_train_features,df_train_features_intermed, on=['city_id','stars'])\n",
    "merged_test = pd.merge(df_test_features,df_test_features_intermed, on=['city_id','stars'])\n",
    "df_train_features['price_diff']=merged_train['avg_price'] - merged_train['area_star_avg_price']\n",
    "df_test_features['price_diff']=merged_test['avg_price'] - merged_test['area_star_avg_price']\n",
    "df_train_features['promotion'] = np.where(df_train_features['price_diff']>0, 1, 0)\n",
    "df_test_features['promotion'] = np.where(df_test_features['price_diff']>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features['n_images_outlier'] = np.where(df_train_features['n_images']>df_train_features['n_images'].std()*2, df_train_features['n_images'].std()*2, df_train_features['n_images'])\n",
    "df_test_features['n_images_outlier'] = np.where(df_test_features['n_images']>df_test_features['n_images'].std()*2, df_test_features['n_images'].std()*2, df_test_features['n_images'])\n",
    "\n",
    "df_train_features['n_reviews_outlier'] = np.where(df_train_features['n_reviews']>df_train_features['n_reviews'].std()*2, df_train_features['n_reviews'].std()*2, df_train_features['n_reviews'])\n",
    "df_test_features['n_reviews_outlier'] = np.where(df_test_features['n_reviews']>df_test_features['n_reviews'].std()*2, df_test_features['n_reviews'].std()*2, df_test_features['n_reviews'])\n",
    "\n",
    "df_train_features['distance_to_center_outlier'] = np.where(df_train_features['distance_to_center']>df_train_features['distance_to_center'].std()*2, df_train_features['distance_to_center'].std()*2, df_train_features['distance_to_center'])\n",
    "df_test_features['distance_to_center_outlier'] = np.where(df_test_features['distance_to_center']>df_test_features['distance_to_center'].std()*2, df_test_features['distance_to_center'].std()*2, df_test_features['distance_to_center'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Starting with a linear model to understand the complexity of the problem to see if a line can fit the data well or not. I fit some linear model and non linear models with default hyperparameter setttng to undertsnd if problem is linear or non-linear. Once we see if linear/non-linear could explain the problem better then we can playaround with different hyperparameters to get better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Linear Regression: \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "# Tuning hyper-parameters for mean_squared_error\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=True .................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=True, score=-13277.225668659068, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=True .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  copy_X=True, fit_intercept=True, normalize=True, score=-12882.234783967644, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=True .................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=True, score=-16049.898364995817, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=True .................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=True, score=-15009.905690041036, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=True .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  copy_X=True, fit_intercept=True, normalize=True, score=-13799.872206993992, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=False ................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=False, score=-13277.225668659068, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=False ................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=False, score=-12882.234783967644, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=False ................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=False, score=-16049.898364995819, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=False ................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=False, score=-15009.905690041036, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=True, normalize=False ................\n",
      "[CV]  copy_X=True, fit_intercept=True, normalize=False, score=-13799.872206993992, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=True ................\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=True, score=-13465.271352429387, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=True ................\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=True, score=-13066.019001649396, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=True ................\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=True, score=-16244.188475939109, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=True ................\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=True, score=-15205.246004771254, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=True ................\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=True, score=-13991.884469134919, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=False ...............\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=False, score=-13465.271352429387, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=False ...............\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=False, score=-13066.019001649396, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=False ...............\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=False, score=-16244.188475939109, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=False ...............\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=False, score=-15205.246004771254, total=   0.1s\n",
      "[CV] copy_X=True, fit_intercept=False, normalize=False ...............\n",
      "[CV]  copy_X=True, fit_intercept=False, normalize=False, score=-13991.884469134919, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=True ................\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=True, score=-13277.225668659068, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=True ................\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=True, score=-12882.234783967644, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=True ................\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=True, score=-16049.898364995817, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=True ................\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=True, score=-15009.905690041036, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=True ................\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=True, score=-13799.872206993992, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=False ...............\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=False, score=-13277.225668659068, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=False ...............\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=False, score=-12882.234783967644, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=False ...............\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=False, score=-16049.898364995819, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=False ...............\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=False, score=-15009.905690041036, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=True, normalize=False ...............\n",
      "[CV]  copy_X=False, fit_intercept=True, normalize=False, score=-13799.872206993992, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=True ...............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=True, score=-13465.271352429387, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=True ...............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=True, score=-13066.019001649396, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=True ...............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=True, score=-16244.188475939109, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=True ...............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=True, score=-15205.246004771254, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=True ...............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=True, score=-13991.884469134919, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=False ..............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=False, score=-13465.271352429387, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=False ..............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=False, score=-13066.019001649396, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=False ..............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=False, score=-16244.188475939109, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=False ..............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=False, score=-15205.246004771254, total=   0.1s\n",
      "[CV] copy_X=False, fit_intercept=False, normalize=False ..............\n",
      "[CV]  copy_X=False, fit_intercept=False, normalize=False, score=-13991.884469134919, total=   0.1s\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'copy_X': True, 'fit_intercept': True, 'normalize': True}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-14203.826 (+/-2336.861) for {'copy_X': True, 'fit_intercept': True, 'normalize': True}\n",
      "-14203.826 (+/-2336.861) for {'copy_X': True, 'fit_intercept': True, 'normalize': False}\n",
      "-14394.521 (+/-2344.204) for {'copy_X': True, 'fit_intercept': False, 'normalize': True}\n",
      "-14394.521 (+/-2344.204) for {'copy_X': True, 'fit_intercept': False, 'normalize': False}\n",
      "-14203.826 (+/-2336.861) for {'copy_X': False, 'fit_intercept': True, 'normalize': True}\n",
      "-14203.826 (+/-2336.861) for {'copy_X': False, 'fit_intercept': True, 'normalize': False}\n",
      "-14394.521 (+/-2344.204) for {'copy_X': False, 'fit_intercept': False, 'normalize': True}\n",
      "-14394.521 (+/-2344.204) for {'copy_X': False, 'fit_intercept': False, 'normalize': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tuned_parameters = {'fit_intercept':[True,False], \n",
    "                    'normalize':[True,False], \n",
    "                    'copy_X':[True, False]}\n",
    "\n",
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)  \n",
    "randomized_search_CV(LinearRegression(),tuned_parameters,norm_X_train,y_train,10,5,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The MSE of the Linear Regression is 16159.950028315048\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The nwmse_metric of the Linear Regression is 1.0402603601922285\n",
      "____________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "\n",
    "model = LinearRegression(copy_X= True, fit_intercept= True, normalize= True)\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "prediction1[prediction1 < 0.9]=0\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the Linear Regression is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the Linear Regression is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "tuned_parameters = {'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100],\n",
    "                    'degree':[3,4,6]}\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "\n",
    "model = SVR(kernel= 'rbf',verbose=10)\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the Support Vector Regressor is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the Support Vector Regressor is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Random Forest Regression:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First choice of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "# Tuning hyper-parameters for mean_squared_error\n",
      "\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  50 | elapsed:  6.1min remaining: 55.3min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  50 | elapsed:  7.1min remaining: 25.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  50 | elapsed:  7.4min remaining: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  50 | elapsed:  7.7min remaining:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  50 | elapsed:  7.8min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  50 | elapsed:  7.8min remaining:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  7.9min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:  7.9min remaining:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'min_samples_leaf': 4, 'max_depth': 10, 'bootstrap': True, 'n_estimators': 400, 'min_samples_split': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-17085.825 (+/-3545.953) for {'min_samples_leaf': 1, 'max_depth': 9, 'bootstrap': False, 'n_estimators': 400, 'min_samples_split': 2}\n",
      "-17250.143 (+/-4432.623) for {'min_samples_leaf': 1, 'max_depth': 10, 'bootstrap': False, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-12062.540 (+/-2274.277) for {'min_samples_leaf': 4, 'max_depth': 10, 'bootstrap': True, 'n_estimators': 400, 'min_samples_split': 10}\n",
      "-12593.085 (+/-2156.531) for {'min_samples_leaf': 2, 'max_depth': 7, 'bootstrap': True, 'n_estimators': 100, 'min_samples_split': 2}\n",
      "-12562.488 (+/-2179.114) for {'min_samples_leaf': 4, 'max_depth': 7, 'bootstrap': True, 'n_estimators': 100, 'min_samples_split': 2}\n",
      "-12591.657 (+/-2177.844) for {'min_samples_leaf': 2, 'max_depth': 7, 'bootstrap': True, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-15808.966 (+/-2713.561) for {'min_samples_leaf': 1, 'max_depth': 8, 'bootstrap': False, 'n_estimators': 300, 'min_samples_split': 10}\n",
      "-14248.227 (+/-2026.841) for {'min_samples_leaf': 4, 'max_depth': 7, 'bootstrap': False, 'n_estimators': 150, 'min_samples_split': 10}\n",
      "-12679.781 (+/-2237.752) for {'min_samples_leaf': 1, 'max_depth': 7, 'bootstrap': True, 'n_estimators': 300, 'min_samples_split': 10}\n",
      "-14271.267 (+/-1992.355) for {'min_samples_leaf': 4, 'max_depth': 7, 'bootstrap': False, 'n_estimators': 150, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# A parameter grid for RandomForestRegressor\n",
    "tuned_parameters = {\n",
    "        'n_estimators': [100,150,200,300,400],\n",
    "        'bootstrap': [True, False],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_depth': [7,8,9,10]\n",
    "        }\n",
    "        \n",
    "\n",
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)  \n",
    "randomized_search_CV(RandomForestRegressor(),tuned_parameters,norm_X_train,y_train,10,10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The MSE of the RandomForestRegressor is 13582.000381017378\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The nwmse_metric of the RandomForestRegressor is 0.8561344180764782\n",
      "____________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=400,bootstrap='True',min_samples_split=10,max_depth=10,min_samples_leaf=4)\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the RandomForestRegressor is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the RandomForestRegressor is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second choice of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "# Tuning hyper-parameters for mean_squared_error\n",
      "\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:  6.1min remaining:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  20 | elapsed:  6.2min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'bootstrap': True, 'n_estimators': 200}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-12150.826 (+/-2008.308) for {'bootstrap': True, 'n_estimators': 200}\n",
      "-12152.645 (+/-2037.654) for {'bootstrap': True, 'n_estimators': 300}\n",
      "-23495.796 (+/-4512.250) for {'bootstrap': False, 'n_estimators': 200}\n",
      "-23522.771 (+/-4511.406) for {'bootstrap': False, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {\n",
    "        'n_estimators': [200,300],\n",
    "        'bootstrap': [True, False],\n",
    "        }\n",
    "\n",
    "\n",
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)  \n",
    "randomized_search_CV(RandomForestRegressor(),tuned_parameters,norm_X_train,y_train,5,3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The MSE of the RandomForestRegressor is 13568.332220455577\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The nwmse_metric of the RandomForestRegressor is 0.8208031777639037\n",
      "____________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=200,bootstrap='True')\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the RandomForestRegressor is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the RandomForestRegressor is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "tuned_parameters = {\n",
    "        'n_estimators': [150,200,300],\n",
    "        'min_child_weight': [3,4],\n",
    "        'gamma':  [0],\n",
    "        'subsample': [ 0.7, 0.8],\n",
    "        'colsample_bytree': [ 0.7, 0.8],\n",
    "        'max_depth': [7,8],\n",
    "        'learning_rate' : [ 0.05, 0.1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for mean_squared_error\n",
      "\n",
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'learning_rate': 0.05, 'gamma': 0, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.7, 'n_estimators': 300, 'min_child_weight': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-11195.440 (+/-1489.591) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 8, 'colsample_bytree': 0.7, 'subsample': 0.7, 'n_estimators': 150, 'min_child_weight': 4}\n",
      "-11241.382 (+/-1529.995) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.8, 'n_estimators': 200, 'min_child_weight': 4}\n",
      "-11197.603 (+/-1567.835) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 4}\n",
      "-11106.935 (+/-1686.429) for {'learning_rate': 0.05, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.7, 'n_estimators': 200, 'min_child_weight': 4}\n",
      "-11051.470 (+/-1555.710) for {'learning_rate': 0.05, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.7, 'n_estimators': 200, 'min_child_weight': 3}\n",
      "-11195.642 (+/-1695.611) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.7, 'n_estimators': 300, 'min_child_weight': 3}\n",
      "-11114.032 (+/-1174.372) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.7, 'n_estimators': 150, 'min_child_weight': 3}\n",
      "-11328.281 (+/-1583.311) for {'learning_rate': 0.1, 'gamma': 0, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.8, 'n_estimators': 150, 'min_child_weight': 4}\n",
      "-10922.398 (+/-1407.672) for {'learning_rate': 0.05, 'gamma': 0, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.7, 'n_estimators': 300, 'min_child_weight': 3}\n",
      "-11228.916 (+/-1777.889) for {'learning_rate': 0.05, 'gamma': 0, 'max_depth': 8, 'colsample_bytree': 0.7, 'subsample': 0.7, 'n_estimators': 150, 'min_child_weight': 4}\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)  \n",
    "randomized_search_CV(XGBRegressor(),tuned_parameters,norm_X_train,y_train,10,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The MSE of the XGBoost is 12339.29996349272\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The nwmse_metric of the Linear Regression is 0.7467574712517664\n",
      "____________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "model = XGBRegressor(n_estimators=300,max_depth=8,eta=0.05,subsample=0.7,colsample_bytree=0.8,min_child_weight=3,gamma=0)\n",
    "\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the XGBoost is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))l\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the XGBoost is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some other Hyperparameters which did'nt came from RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (316608, 12)\n",
      "shape of y_train: (316608,)\n",
      "shape of x_valid: (79152, 12)\n",
      "shape of y_valid: (79152,)\n",
      "shape of x_test: (131929, 12)\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The MSE of the XGBoost is 11945.49938706277\n",
      "____________________________________________________________________________________________________ \n",
      "\n",
      "The nwmse_metric of the XGBoost is 0.7226584539113659\n",
      "____________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=['content_score', 'n_images_outlier', 'distance_to_center_outlier', 'avg_rating', 'stars', 'n_reviews_outlier', 'avg_rank','avg_price','avg_saving_percent','n_hotels','hotel_popularity_reviews','promotion']\n",
    "norm_X_train,y_train,norm_X_valid,y_valid,norm_X_test=prepare_data(df_train_features.fillna(0),df_test_features.fillna(0),features)\n",
    "model = XGBRegressor(n_estimators=300,max_depth=7,eta=0.1,subsample=0.8,colsample_bytree=0.8,min_child_weight=3)\n",
    "\n",
    "model.fit(norm_X_train,y_train)\n",
    "prediction1=model.predict(norm_X_valid)\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The R2 of the Linear Regression is',r2_score(y_valid,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The MSE of the XGBoost is',mean_squared_error(y_valid,prediction1))\n",
    "#print(\"__\"*50,\"\\n\")\n",
    "#print('The MAE of the Linear Regression is',mean_absolute_error(y_valid,prediction1))l\n",
    "print(\"__\"*50,\"\\n\")\n",
    "print('The nwmse_metric of the XGBoost is',nwmse_metric(y_valid.values,prediction1))\n",
    "print(\"__\"*50,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1 avg_rank, denoted by number 6: 0.18312442\n",
      "2 avg_price, denoted by number 7: 0.12671965\n",
      "3 n_hotels, denoted by number 9: 0.124732494\n",
      "4 n_reviews_outlier, denoted by number 5: 0.11361205\n",
      "5 avg_saving_percent, denoted by number 8: 0.086212166\n",
      "6 distance_to_center_outlier, denoted by number 2: 0.08078569\n",
      "7 content_score, denoted by number 0: 0.0729517\n",
      "8 n_images_outlier, denoted by number 1: 0.06840416\n",
      "9 hotel_popularity_reviews, denoted by number 10: 0.04738612\n",
      "10 avg_rating, denoted by number 3: 0.047309693\n",
      "11 stars, denoted by number 4: 0.041539285\n",
      "12 promotion, denoted by number 11: 0.007222562\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(norm_X_train.shape[1]):\n",
    "    #print(features[indices[f]])\n",
    "    \n",
    "    print(str(f+1)+\" \"+str(features[indices[f]])+\", denoted by number \"+str(indices[f])+\": \"+str(importances[indices[f]]))\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(norm_X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(norm_X_train.shape[1]), indices)\n",
    "plt.xlim([-1, norm_X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for test data and saving in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_data=model.predict(norm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features['n_clicks']=prediction_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131929, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res=df_test_features[['hotel_id','n_clicks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>n_clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14942256073</td>\n",
       "      <td>2.676275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16036037903</td>\n",
       "      <td>3.931976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288585940112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129041645070</td>\n",
       "      <td>1.384049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12460296563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hotel_id  n_clicks\n",
       "0   14942256073  2.676275\n",
       "1   16036037903  3.931976\n",
       "2  288585940112  0.000000\n",
       "3  129041645070  1.384049\n",
       "4   12460296563  0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131929, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res.to_csv(\"test_data_results_bhansali.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps:\n",
    "***\n",
    "- Since 65% of rows have 'n_clicks' values to be 0. We can 2 models in following way:\n",
    "    \n",
    "    - Build a \"classification model\" which classifies rows as a 0 clicks or non zero clicks (lablel each row as 0 or non zero)\n",
    "        \n",
    "    - The rows which classification model classifies as non-zero clicks, build a \"regression model\" on those data to predict n_clicks.\n",
    "    \n",
    "- I have not used 'city_id' as of now directly in the set of features. However, we can learn embedding for the city_id's. In this way, cities with similar \"hotel characterstics\" will be near in feature space (similar embeddings). For example, cities with similar kind of hotels (stars) and good ratings are near in the embedding space. We could use this as one of the feature. This could be done using neural network (simple multi layer perceptron model).\n",
    "        \n",
    "- Some sampling techniques or weighting technique could be used to balance the dataset (zero and non-zero n_clicks) so that model is not biased to n_clicks=0.\n",
    "   \n",
    "- Some more features could be thought on by understanding more business domain.\n",
    "   \n",
    "- As of now, I have just used simple technique to remove \"outliers\" without business knowledge. Some other techniques like z-score (parametric model could be applied which says how many standard deviations a data point is from the samples mean).  \n",
    "***        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
